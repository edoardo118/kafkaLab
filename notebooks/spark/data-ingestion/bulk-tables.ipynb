{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 2px;\">\n",
    "  <img src=\"https://www.quantiaconsulting.com/logos/quantia_logo_orizz.png\" alt=\"Quantia Consulting\" style=\"width: 600px; height: 250px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Reading Data with Spark - Tables\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "- Demonstrate how to pre-register data sources in HIVE Data Warehouse.\n",
    "- Introduce temporary views over files.\n",
    "- Read data from tables/views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Let's start importing libraries and creating useful variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import os\n",
    "import qcutils\n",
    "from pyspark.sql import SparkSession\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "baseUri = \"s3a://quantia-master/training/\"\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.5 pyspark-shell'\n",
    "\n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "qcutils.init_spark_session(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering Tables\n",
    "\n",
    "So far we've seen purely programmatic methods for reading in data.\n",
    "\n",
    "Spark allows us to \"register\" the equivalent of \"tables\" so that they can be easily accessed by all users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register a Table/View\n",
    "* An in-memory DataFrame can be easily persisted as a table using the `saveAsTable(...)` command\n",
    "* In our case we are going to to persiste the file `wikipedia_pageviews_by_second.parquet`, a parquet file we are already able to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = baseUri + \"wikipedia_pageviews_by_second.parquet\"\n",
    "df = spark.read.parquet(parquetFile)\n",
    "df.write.saveAsTable(\"pageviews_by_second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** If a table already esists, you must specify the writing `mode`. More details in the Writing Data notebook.\n",
    "\n",
    "The writing `mode` can be `overwrite`, `append`, `ErrorIfExists` or `ignore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"pageviews_by_second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from a Table/View\n",
    "\n",
    "We can now read in the \"table\" **pageviews_by_seconds_example** as a `DataFrame` with one simple command (and then print the schema):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageviewsBySecondsExampleDF = spark.read.table(\"pageviews_by_second\")\n",
    "\n",
    "pageviewsBySecondsExampleDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course we can now view that data as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageviewsBySecondsExampleDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: Reading from Tables\n",
    "* No job is executed - the schema is stored in the table definition in local HIVE Data Warehouse.\n",
    "* The data types shown here are those we defined when we registered the table.\n",
    "* In our case, the file was locally on the cluster.\n",
    "* The \"registration\" of the table simply makes future access, or access by multiple users easier.\n",
    "* The users of the notebook cannot see username and passwords, secret keys, tokens, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary Views\n",
    "\n",
    "Tables that are loadable by the call `spark.read.table(..)` are also accessible through the SQL APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from pageviews_by_second limit(5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also take an existing `DataFrame` and register it as a view exposing it as a table to the SQL API.\n",
    "\n",
    "If you recall from earlier, we have an instance called `parquetDF`.\n",
    "\n",
    "We can create a [temporary] view with this call..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temporary view from the resulting DataFrame\n",
    "pageviewsBySecondsExampleDF.createOrReplaceTempView(\"temp_pageviews_by_second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can use the SQL API to reference that same `DataFrame` as the table **parquet_table**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from temp_pageviews_by_second limit(5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Notes** \n",
    "\n",
    "The method createOrReplaceTempView(..) is bound to the SparkSession meaning it will be discarded once the session ends.\n",
    "\n",
    "On the other hand, the method createOrReplaceGlobalTempView(..) is bound to the spark application.\n",
    "\n",
    "Or to put that another way, I can use createOrReplaceTempView(..) in this notebook only. However, I can call createOrReplaceGlobalTempView(..) in this notebook and then access it from another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ![Quantia Tiny Logo](https://www.quantiaconsulting.com/logos/quantia_logo_tiny.png) 2020 Quantia Consulting, srl. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "Reading Data - Tables",
  "notebookId": 1507370365633334
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
