{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 2px;\">\n",
    "  <img src=\"https://www.quantiaconsulting.com/logos/quantia_logo_orizz.png\" alt=\"Quantia Consulting\" style=\"width: 600px; height: 250px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark: Handle Corrupt/bad Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time writing ingestion jobs becomes very expensive when it comes to handling corrupt records. And in such cases, ingestion pipelines need a good solution to handle corrupted records. Because, larger the ingestion pipeline is, the more complex it becomes to handle such bad records in between. Corrupt data includes:\n",
    "* Missing information\n",
    "* Incomplete information\n",
    "* Schema mismatch\n",
    "* Differing formats or data types\n",
    "\n",
    "Since ingestion pipelines are built to be automated, production-oriented solutions must ensure pipelines behave as expected. This means that data engineers must both expect and systematically handle corrupt records.\n",
    "\n",
    "So, before proceeding to our main topic, let’s first know the pathway to ingestion pipeline & where comes the step to handle corrupted records.\n",
    "\n",
    "![](https://www.quantiaconsulting.com/img/ETL-Process-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As, it is clearly visible that just before loading the final result, it is a good practice to handle corrupted/bad records. Now, the main question arises is How to handle corrupted/bad records? So, here comes the answer to the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "baseUri = \"/home/jovyan/materials/local-data/\"\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.5 pyspark-shell'\n",
    "\n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's recall the \"all-good\" case\n",
    "\n",
    "To answer this question, we will see a complete example about how to play & handle the bad record present in JSON. But, let's first recall the \"all-good\" case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say this is the **good** JSON data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\"a\": 1, \"b\":2, \"c\":3}\n",
    "{\"a\": 10, \"b\":20, \"c\":30}\n",
    "{\"a\": 100, \"b\":30, \"c\":300}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodDf = (spark.read.json(baseUri+\"good.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Corrupt/bad records\n",
    "\n",
    "Let’s say this is the **bad** JSON data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\"a\": 1, \"b\":2, \"c\":3}\n",
    "{\"a\": 10, \"b\":20, \"c\":30}\n",
    "{\"a\": 100, \"b, \"c\":300}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above JSON data `{\"a\": 3, \"b, \"c\":300}` is the bad record. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the main target is how to handle this record?\n",
    "\n",
    "We have three ways to handle this type of data-\n",
    "A) To include this data in a separate column\n",
    "B) To ignore all bad records\n",
    "C) Throws an exception when it meets corrupted records\n",
    "\n",
    "So, let’s see each of these 3 ways in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) To include this data in a separate column\n",
    "\n",
    "As per the use case, if a user wants us to store a bad record in separate column use option mode as `PERMISSIVE`. \n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptDf = (spark.read\n",
    "             .option(\"mode\", \"PERMISSIVE\")\n",
    "             .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "             .json(baseUri+\"bad.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's displayed `corruptDf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: the `_corrupt_record` column only appears if there is at least 1 corrupt record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.read\n",
    " .option(\"mode\", \"PERMISSIVE\")\n",
    " .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    " .json(baseUri+\"good.json\")\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many corrupt records are there?\n",
    "\n",
    "Directly counting them gives an error ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "corruptDf.filter(col(\"_corrupt_record\").isNotNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note the error**: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\n",
    "referenced columns only include the internal corrupt record column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to query the corrupt record column you need to cache the dataframe. In this way you explicitly tell that you are aware that the data are corrupt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badRows = corruptDf.filter(col(\"_corrupt_record\").isNotNull())\n",
    "badRows.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badRows.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) To ignore all bad records \n",
    "\n",
    "In this particular use case, if a user doesn’t want to include the bad records at all and wants to store only the correct records use the `DROPMALFORMED` mode.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDf = (spark.read\n",
    "             .option(\"mode\", \"DROPMALFORMED\")\n",
    "             .json(baseUri+\"bad.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's displayed `cleanDf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, only the correct records will be stored & bad records will be removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Throws an exception when it meets corrupted records\n",
    "\n",
    "For this use case, if present any bad record will throw an exception. And the mode for this use case will be `FAILFAST`. And it’s a best practice to use this mode in a try-catch block.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    anotherCorruptDf = (spark.read\n",
    "        .option(\"mode\", \"FAILFAST\")\n",
    "        .json(baseUri+\"bad.json\")\n",
    "    )\n",
    "except Exception as e:  \n",
    "    print(e)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, will throw an error and no data is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anotherCorruptDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements \n",
    "\n",
    "This notebook is partially based on [Apache Spark: Handle Corrupt/Bad Records by Divyansh Jain, published onApril 5, 2020](https://blog.knoldus.com/apache-spark-handle-corrupt-bad-records/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ![Quantia Tiny Logo](https://www.quantiaconsulting.com/logos/quantia_logo_tiny.png) Quantia Consulting, srl. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}