{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 2px;\">\n",
    "  <img src=\"https://www.quantiaconsulting.com/logos/quantia_logo_orizz.png\" alt=\"Quantia Consulting\" style=\"width: 600px; height: 250px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Reading Data in Spark - JSON Files\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "- Read data from:\n",
    "  * JSON without a Schema\n",
    "  * JSON with a Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Let's start importing libraries and creating useful variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import os\n",
    "import qcutils\n",
    "from pyspark.sql import SparkSession\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "baseUri = \"s3a://quantia-master/training/\"\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.5 pyspark-shell'\n",
    "\n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "qcutils.init_spark_session(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from JSON w/ InferSchema\n",
    "\n",
    "Reading in JSON isn't that much different than reading in CSV files.\n",
    "\n",
    "Let's start with taking a look at all the different options that go along with reading in JSON files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Lines\n",
    "\n",
    "Much like the CSV reader, the JSON reader also assumes...\n",
    "* That there is one JSON object per line and...\n",
    "* That it's delineated by a new-line.\n",
    "\n",
    "This format is referred to as **JSON Lines** or **newline-delimited JSON** \n",
    "\n",
    "More information about this format can be found at <a href=\"http://jsonlines.org/\" target=\"_blank\">http://jsonlines.org</a>.\n",
    "\n",
    "**Notes:** *Spark 2.2 was released on July 11th 2016. With that comes File IO improvements for CSV & JSON, but more importantly, **Support for parsing multi-line JSON and CSV files**. You can read more about that (and other features in Spark 2.2) in the <a href=\"https://databricks.com/blog/2017/07/11/introducing-apache-spark-2-2.html\" target=\"_blank\">Databricks Blog</a>.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data Source\n",
    "* For this exercise, we will be using the file called **snapshot-2016-05-26.json** (<a href=\"https://wikitech.wikimedia.org/wiki/Stream.wikimedia.org/rc\" target=\"_blank\">4 MB</a> file from Wikipedia).\n",
    "* The data represents a set of edits to Wikipedia articles captured in May of 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcutils.list_s3_bucket_objects(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcutils.print_s3_bucket_object(key='training/snapshot-2016-05-26.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read The JSON File\n",
    "\n",
    "The command to read in JSON looks very similar to that of CSV.\n",
    "\n",
    "In addition to reading the JSON file, we will also print the resulting schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonFile = baseUri + \"snapshot-2016-05-26.json\"\n",
    "\n",
    "wikiEditsDF = (spark.read           # The DataFrameReader\n",
    "    .option(\"inferSchema\", \"true\")  # Automatically infer data types & column names\n",
    "    .json(jsonFile)                 # Creates a DataFrame from JSON after reading in the file\n",
    " )\n",
    "wikiEditsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our DataFrame created, we can now take a peak at the data.\n",
    "\n",
    "But to demonstrate a unique aspect of JSON data (or any data with embedded fields), we will first create a temporary view and then view the data via SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a view called wiki_edits\n",
    "wikiEditsDF.createOrReplaceTempView(\"wiki_edits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can take a peak at the data with simple SQL SELECT statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM wiki_edits\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the **geocoding** column has embedded data.\n",
    "\n",
    "You can expand the fields by clicking the right triangle in each row.\n",
    "\n",
    "But we can also reference the sub-fields directly as we see in the following SQL statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT channel, page, geocoding.city, geocoding.latitude, geocoding.longitude FROM wiki_edits WHERE geocoding.city IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: Reading from JSON w/ InferSchema\n",
    "\n",
    "While there are similarities between reading in CSV & JSON there are some key differences:\n",
    "* We only need one job even when inferring the schema.\n",
    "* There is no header which is why there isn't a second job in this case - the column names are extracted from the JSON object's attributes.\n",
    "* Unlike CSV which reads in 100% of the data, the JSON reader only samples the data.  \n",
    "**Note:** In Spark 2.2 the behavior was changed to read in the entire JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from JSON w/ User-Defined Schema\n",
    "\n",
    "To avoid the extra job, we can (just like we did with CSV) specify the schema for the `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #1 - Create the Schema\n",
    "\n",
    "Compared to our CSV example, the structure of this data is a little more complex.\n",
    "\n",
    "Note that we can support complex data types as seen in the field `geocoding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required for StructField, StringType, IntegerType, etc.\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "jsonSchema = StructType([\n",
    "  StructField(\"channel\", StringType(), True),\n",
    "  StructField(\"comment\", StringType(), True),\n",
    "  StructField(\"delta\", IntegerType(), True),\n",
    "  StructField(\"flag\", StringType(), True),\n",
    "  StructField(\"geocoding\", StructType([\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"countryCode2\", StringType(), True),\n",
    "    StructField(\"countryCode3\", StringType(), True),\n",
    "    StructField(\"stateProvince\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True)\n",
    "  ]), True),\n",
    "  StructField(\"isAnonymous\", BooleanType(), True),\n",
    "  StructField(\"isNewPage\", BooleanType(), True),\n",
    "  StructField(\"isRobot\", BooleanType(), True),\n",
    "  StructField(\"isUnpatrolled\", BooleanType(), True),\n",
    "  StructField(\"namespace\", StringType(), True),\n",
    "  StructField(\"page\", StringType(), True),\n",
    "  StructField(\"pageURL\", StringType(), True),\n",
    "  StructField(\"timestamp\", StringType(), True),\n",
    "  StructField(\"url\", StringType(), True),\n",
    "  StructField(\"user\", StringType(), True),\n",
    "  StructField(\"userURL\", StringType(), True),\n",
    "  StructField(\"wikipediaURL\", StringType(), True),\n",
    "  StructField(\"wikipedia\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a lot of typing to get our schema!\n",
    "\n",
    "For a small file, manually creating the the schema may not be worth the effort.\n",
    "\n",
    "However, for a large file, the time to manually create the schema may be worth the trade off of a really long infer-schema process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #2 - Read in the JSON\n",
    "\n",
    "Next, we will read in the JSON file and once again print its schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.read            # The DataFrameReader\n",
    "  .schema(jsonSchema)  # Use the specified schema\n",
    "  .json(jsonFile)      # Creates a DataFrame from JSON after reading in the file\n",
    "  .printSchema()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: Reading from JSON w/ User-Defined Schema\n",
    "* Just like CSV, providing the schema avoids the extra jobs.\n",
    "* The schema allows us to rename columns and specify alternate data types.\n",
    "* Can get arbitrarily complex in its structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ![Quantia Tiny Logo](https://www.quantiaconsulting.com/logos/quantia_logo_tiny.png) 2020 Quantia Consulting, srl. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "name": "Reading Data - JSON",
  "notebookId": 1507370365633214
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
