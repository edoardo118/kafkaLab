{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 2px;\">\n",
    "  <img src=\"https://www.quantiaconsulting.com/logos/quantia_logo_orizz.png\" alt=\"Quantia Consulting\" style=\"width: 600px; height: 250px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Reading Data with Spark - Parquet Files\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "- Introduce the Parquet file format.\n",
    "- Read data from:\n",
    "  - Parquet files without a Schema.\n",
    "  - Parquet files with a Schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Let's start importing libraries and creating useful variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-emanuele-2edellavalle:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2406132390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import os\n",
    "import qcutils\n",
    "from pyspark.sql import SparkSession\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "baseUri = \"s3a://quantia-master/training/\"\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.5 pyspark-shell'\n",
    "\n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "qcutils.init_spark_session(spark)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from Parquet Files\n",
    "\n",
    "[Apache Parquet](https://parquet.apache.org/) is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Parquet Files\n",
    "* Free & Open Source.\n",
    "* Increased query performance over row-based data stores.\n",
    "* Provides efficient data compression.\n",
    "* Designed for performance on large data sets.\n",
    "* Supports limited schema evolution.\n",
    "* Is a splittable \"file format\".\n",
    "* A <a href=\"https://en.wikipedia.org/wiki/Column-oriented_DBMS\" target=\"_blank\">Column-Oriented</a> data store\n",
    "\n",
    "**Row Format** \n",
    "\n",
    "| ID |  Name | Score |\n",
    "|:--:|:-----:|:-----:|\n",
    "| 1  | john  | 4.1   |\n",
    "| 2  | mike  | 3.5   |\n",
    "| 3  | sally | 6.4   |\n",
    "\n",
    "**Columnar View**\n",
    "\n",
    "```\n",
    "ID: 1, 2, 3\n",
    "Name: john, mike, sally\n",
    "Score: 4.1, 3.5, 6.4\n",
    "```\n",
    "\n",
    "**See also**:\n",
    "* [Apache Parquet](https://parquet.apache.org)\n",
    "* [Apache Parquet on Wijipedia](https://en.wikipedia.org/wiki/Apache_Parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "\n",
    "The data for this example shows the traffic to various articles on Wikipedia (<a href=\"https://dumps.wikimedia.org/other/pagecounts-raw\" target=\"_blank\">23 MB</a> from Wikipedia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcutils.list_s3_bucket_objects(limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike our CSV and JSON example, the parquet \"file\" is actually 11 files, 8 of which consist of the bulk of the data and the other three consist of meta-data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Parquet Files\n",
    "\n",
    "To read in this files, we will specify the location of the parquet directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- site: string (nullable = true)\n",
      " |-- requests: integer (nullable = true)\n",
      "\n",
      "time: 4.84 s\n"
     ]
    }
   ],
   "source": [
    "parquetFile = baseUri + \"wikipedia_pageviews_by_second.parquet\"\n",
    "\n",
    "(spark.read              # The DataFrameReader\n",
    "  .parquet(parquetFile)  # Creates a DataFrame from Parquet after reading in the file\n",
    "  .printSchema()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>timestamp</th><th>site</th><th>requests</th></tr>\n",
       "<tr><td>2015-03-22T14:13:34</td><td>mobile</td><td>1425</td></tr>\n",
       "<tr><td>2015-03-22T14:23:18</td><td>desktop</td><td>2534</td></tr>\n",
       "<tr><td>2015-03-22T14:36:47</td><td>desktop</td><td>2444</td></tr>\n",
       "<tr><td>2015-03-22T14:38:39</td><td>mobile</td><td>1488</td></tr>\n",
       "<tr><td>2015-03-22T14:57:11</td><td>mobile</td><td>1519</td></tr>\n",
       "<tr><td>2015-03-22T15:03:18</td><td>mobile</td><td>1559</td></tr>\n",
       "<tr><td>2015-03-22T15:16:47</td><td>mobile</td><td>1510</td></tr>\n",
       "<tr><td>2015-03-22T15:45:03</td><td>desktop</td><td>2673</td></tr>\n",
       "<tr><td>2015-03-22T15:58:32</td><td>desktop</td><td>2463</td></tr>\n",
       "<tr><td>2015-03-22T16:06:11</td><td>desktop</td><td>2525</td></tr>\n",
       "<tr><td>2015-03-22T16:19:40</td><td>desktop</td><td>2441</td></tr>\n",
       "<tr><td>2015-03-22T16:20:24</td><td>desktop</td><td>2781</td></tr>\n",
       "<tr><td>2015-03-22T16:25:03</td><td>mobile</td><td>1842</td></tr>\n",
       "<tr><td>2015-03-22T16:33:53</td><td>desktop</td><td>2549</td></tr>\n",
       "<tr><td>2015-03-22T16:38:32</td><td>mobile</td><td>1666</td></tr>\n",
       "<tr><td>2015-03-22T16:45:29</td><td>desktop</td><td>2601</td></tr>\n",
       "<tr><td>2015-03-22T16:52:45</td><td>mobile</td><td>1742</td></tr>\n",
       "<tr><td>2015-03-22T16:58:58</td><td>desktop</td><td>2559</td></tr>\n",
       "<tr><td>2015-03-22T17:03:11</td><td>mobile</td><td>1775</td></tr>\n",
       "<tr><td>2015-03-22T17:09:24</td><td>desktop</td><td>2469</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-------------------+-------+--------+\n",
       "|          timestamp|   site|requests|\n",
       "+-------------------+-------+--------+\n",
       "|2015-03-22T14:13:34| mobile|    1425|\n",
       "|2015-03-22T14:23:18|desktop|    2534|\n",
       "|2015-03-22T14:36:47|desktop|    2444|\n",
       "|2015-03-22T14:38:39| mobile|    1488|\n",
       "|2015-03-22T14:57:11| mobile|    1519|\n",
       "|2015-03-22T15:03:18| mobile|    1559|\n",
       "|2015-03-22T15:16:47| mobile|    1510|\n",
       "|2015-03-22T15:45:03|desktop|    2673|\n",
       "|2015-03-22T15:58:32|desktop|    2463|\n",
       "|2015-03-22T16:06:11|desktop|    2525|\n",
       "|2015-03-22T16:19:40|desktop|    2441|\n",
       "|2015-03-22T16:20:24|desktop|    2781|\n",
       "|2015-03-22T16:25:03| mobile|    1842|\n",
       "|2015-03-22T16:33:53|desktop|    2549|\n",
       "|2015-03-22T16:38:32| mobile|    1666|\n",
       "|2015-03-22T16:45:29|desktop|    2601|\n",
       "|2015-03-22T16:52:45| mobile|    1742|\n",
       "|2015-03-22T16:58:58|desktop|    2559|\n",
       "|2015-03-22T17:03:11| mobile|    1775|\n",
       "|2015-03-22T17:09:24|desktop|    2469|\n",
       "+-------------------+-------+--------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.21 s\n"
     ]
    }
   ],
   "source": [
    "parquetFile = baseUri + \"wikipedia_pageviews_by_second.parquet\"\n",
    "\n",
    "(spark.read              # The DataFrameReader\n",
    "  .parquet(parquetFile)  # Creates a DataFrame from Parquet after reading in the file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: Reading from Parquet Files\n",
    "* We do not need to specify the schema - the column names and data types are stored in the parquet files.\n",
    "* Only one job is required to **read** that schema from the parquet file's metadata.\n",
    "* Unlike the CSV or JSON readers that have to load the entire file and then infer the schema, the parquet reader can \"read\" the schema very quickly because it's reading that schema from the metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Parquet Files w/Schema\n",
    "\n",
    "If you want to avoid the extra job entirely, we can, again, specify the schema even for parquet files:\n",
    "\n",
    "** *WARNING* ** *Providing a schema may avoid this one-time hit to determine the `DataFrame's` schema.*  \n",
    "*However, if you specify the wrong schema it will conflict with the true schema and will result in an analysis exception at runtime.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- site: string (nullable = true)\n",
      " |-- requests: integer (nullable = true)\n",
      "\n",
      "time: 283 ms\n"
     ]
    }
   ],
   "source": [
    "# Required for StructField, StringType, IntegerType, etc.\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "parquetSchema = StructType(\n",
    "  [\n",
    "    StructField(\"timestamp\", StringType(), False),\n",
    "    StructField(\"site\", StringType(), False),\n",
    "    StructField(\"requests\", IntegerType(), False)\n",
    "  ]\n",
    ")\n",
    "\n",
    "(spark.read          # The DataFrameReader\n",
    "  .schema(parquetSchema)  # Use the specified schema\n",
    "  .parquet(parquetFile)   # Creates a DataFrame from Parquet after reading in the file\n",
    "  .printSchema()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's have a look to the execution time for reading 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>timestamp</th><th>site</th><th>requests</th></tr>\n",
       "<tr><td>2015-03-22T14:13:34</td><td>mobile</td><td>1425</td></tr>\n",
       "<tr><td>2015-03-22T14:23:18</td><td>desktop</td><td>2534</td></tr>\n",
       "<tr><td>2015-03-22T14:36:47</td><td>desktop</td><td>2444</td></tr>\n",
       "<tr><td>2015-03-22T14:38:39</td><td>mobile</td><td>1488</td></tr>\n",
       "<tr><td>2015-03-22T14:57:11</td><td>mobile</td><td>1519</td></tr>\n",
       "<tr><td>2015-03-22T15:03:18</td><td>mobile</td><td>1559</td></tr>\n",
       "<tr><td>2015-03-22T15:16:47</td><td>mobile</td><td>1510</td></tr>\n",
       "<tr><td>2015-03-22T15:45:03</td><td>desktop</td><td>2673</td></tr>\n",
       "<tr><td>2015-03-22T15:58:32</td><td>desktop</td><td>2463</td></tr>\n",
       "<tr><td>2015-03-22T16:06:11</td><td>desktop</td><td>2525</td></tr>\n",
       "<tr><td>2015-03-22T16:19:40</td><td>desktop</td><td>2441</td></tr>\n",
       "<tr><td>2015-03-22T16:20:24</td><td>desktop</td><td>2781</td></tr>\n",
       "<tr><td>2015-03-22T16:25:03</td><td>mobile</td><td>1842</td></tr>\n",
       "<tr><td>2015-03-22T16:33:53</td><td>desktop</td><td>2549</td></tr>\n",
       "<tr><td>2015-03-22T16:38:32</td><td>mobile</td><td>1666</td></tr>\n",
       "<tr><td>2015-03-22T16:45:29</td><td>desktop</td><td>2601</td></tr>\n",
       "<tr><td>2015-03-22T16:52:45</td><td>mobile</td><td>1742</td></tr>\n",
       "<tr><td>2015-03-22T16:58:58</td><td>desktop</td><td>2559</td></tr>\n",
       "<tr><td>2015-03-22T17:03:11</td><td>mobile</td><td>1775</td></tr>\n",
       "<tr><td>2015-03-22T17:09:24</td><td>desktop</td><td>2469</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-------------------+-------+--------+\n",
       "|          timestamp|   site|requests|\n",
       "+-------------------+-------+--------+\n",
       "|2015-03-22T14:13:34| mobile|    1425|\n",
       "|2015-03-22T14:23:18|desktop|    2534|\n",
       "|2015-03-22T14:36:47|desktop|    2444|\n",
       "|2015-03-22T14:38:39| mobile|    1488|\n",
       "|2015-03-22T14:57:11| mobile|    1519|\n",
       "|2015-03-22T15:03:18| mobile|    1559|\n",
       "|2015-03-22T15:16:47| mobile|    1510|\n",
       "|2015-03-22T15:45:03|desktop|    2673|\n",
       "|2015-03-22T15:58:32|desktop|    2463|\n",
       "|2015-03-22T16:06:11|desktop|    2525|\n",
       "|2015-03-22T16:19:40|desktop|    2441|\n",
       "|2015-03-22T16:20:24|desktop|    2781|\n",
       "|2015-03-22T16:25:03| mobile|    1842|\n",
       "|2015-03-22T16:33:53|desktop|    2549|\n",
       "|2015-03-22T16:38:32| mobile|    1666|\n",
       "|2015-03-22T16:45:29|desktop|    2601|\n",
       "|2015-03-22T16:52:45| mobile|    1742|\n",
       "|2015-03-22T16:58:58|desktop|    2559|\n",
       "|2015-03-22T17:03:11| mobile|    1775|\n",
       "|2015-03-22T17:09:24|desktop|    2469|\n",
       "+-------------------+-------+--------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "(spark.read          # The DataFrameReader\n",
    "  .schema(parquetSchema)  # Use the specified schema\n",
    "  .parquet(parquetFile)   # Creates a DataFrame from Parquet after reading in the file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on the time to count when the source is a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7200000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(parquetFile).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took approximately 1 sec. If you remember, the same task took around 3 sec when the source was a file. **It is almost 3 times faster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>sum(requests)</th></tr>\n",
       "<tr><td>13342978934</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------+\n",
       "|sum(requests)|\n",
       "+-------------+\n",
       "|  13342978934|\n",
       "+-------------+"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(parquetFile)\n",
    "df.select(df.requests).groupBy().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took approximately 1.5 sec. If you remember, the same task took around 9 sec when the source was a file. **It is 6 times faster**.\n",
    "\n",
    "#### Discussion\n",
    "* Why is counting 3x?\n",
    "* Why is summing a column even faster (6x)?\n",
    "* What do you expect if the column were 100 instead of 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ![Quantia Tiny Logo](https://www.quantiaconsulting.com/logos/quantia_logo_tiny.png) 2020 Quantia Consulting, srl. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "Reading Data - Parquet",
  "notebookId": 1507370365633178
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
