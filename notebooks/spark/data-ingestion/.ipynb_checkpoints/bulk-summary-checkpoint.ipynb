{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 2px;\">\n",
    "  <img src=\"https://www.quantiaconsulting.com/logos/quantia_logo_orizz.png\" alt=\"Quantia Consulting\" style=\"width: 600px; height: 250px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Reading Data - Summary (+ Little More)\n",
    "\n",
    "In this notebook, we will quickly compare the various methods for reading in data and present the Columnar Predicate Pushdown.\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "- Contrast the various techniques for reading data.\n",
    "- Understand the Columnar Predicate Pushdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General\n",
    "- `SparkSession` is our entry point for working with the `DataFrames` API\n",
    "- `DataFrameReader` is the interface to the various read operations\n",
    "- Each reader behaves differently when it comes to the number of initial partitions and depends on both the file format (CSV vs Parquet vs ORC) and the source (Azure Blob vs Amazon S3 vs JDBC vs HDFS)\n",
    "- Ultimately, it is dependent on the implementation of the `DataFrameReader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "| Type    | <span style=\"white-space:nowrap\">Inference Type</span> | <span style=\"white-space:nowrap\">Inference Speed</span> | Reason                                          | <span style=\"white-space:nowrap\">Should Supply Schema?</span> |\n",
    "|---------|--------------------------------------------------------|---------------------------------------------------------|----------------------------------------------------|:--------------:|\n",
    "| <b>CSV</b>     | <span style=\"white-space:nowrap\">Full-Data-Read</span> | <span style=\"white-space:nowrap\">Slow</span>            | <span style=\"white-space:nowrap\">File size</span>  | Yes            |\n",
    "| <b>Parquet</b> | <span style=\"white-space:nowrap\">Metadata-Read</span>  | <span style=\"white-space:nowrap\">Fast/Medium</span>     | <span style=\"white-space:nowrap\">Number of Partitions</span> | No (most cases)             |\n",
    "| <b>Tables</b>  | <span style=\"white-space:nowrap\">n/a</span>            | <span style=\"white-space:nowrap\">n/a</span>            | <span style=\"white-space:nowrap\">Predefined</span> | n/a            |\n",
    "| <b>JSON</b>    | <span style=\"white-space:nowrap\">Full-Read-Data</span> | <span style=\"white-space:nowrap\">Slow</span>            | <span style=\"white-space:nowrap\">File size</span>  | Yes            |\n",
    "| <b>Text</b>    | <span style=\"white-space:nowrap\">Dictated</span>       | <span style=\"white-space:nowrap\">Zero</span>            | <span style=\"white-space:nowrap\">Only 1 Column</span>   | Never          |\n",
    "| <b>JDBC</b>    | <span style=\"white-space:nowrap\">DB-Read</span>        | <span style=\"white-space:nowrap\">Fast</span>            | <span style=\"white-space:nowrap\">DB Schema</span>  | No             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CSV\n",
    "- `spark.read.csv(..)`\n",
    "- There are a large number of options when reading CSV files including headers, column separator, escaping, etc.\n",
    "- We can allow Spark to infer the schema at the cost of first reading in the entire file.\n",
    "- Large CSV files should always have a schema pre-defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Parquet\n",
    "- `spark.read.parquet(..)`\n",
    "- Parquet files are the preferred file format for big-data.\n",
    "- It is a columnar file format.\n",
    "- It is a splittable file format.\n",
    "- It offers a lot of performance benefits over other formats including predicate pushdown.\n",
    "- Unlike CSV, the schema is read in, not inferred.\n",
    "- Reading the schema from Parquet's metadata can be extremely efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Tables\n",
    "- `spark.read.table(..)`\n",
    "- The Databricks platform allows us to register a huge variety of data sources as tables via the Databricks UI.\n",
    "- Any `DataFrame` (from CSV, Parquet, whatever) can be registered as a temporary view.\n",
    "- Tables/Views can be loaded via the `DataFrameReader` to produce a `DataFrame`\n",
    "- Tables/Views can be used directly in SQL statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading JSON\n",
    "- `spark.read.json(..)`\n",
    "- JSON represents complex data types unlike CSV's flat format.\n",
    "- Has many of the same limitations as CSV (needing to read the entire file to infer the schema)\n",
    "- Like CSV has a lot of options allowing control on date formats, escaping, single vs. multiline JSON, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Text\n",
    "- `spark.read.text(..)`\n",
    "- Reads one line of text as a single column named `value`.\n",
    "- Is the basis for more complex file formats such as fixed-width text files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading JDBC\n",
    "- `spark.read.jdbc(..)`\n",
    "- Requires one database connection per partition.\n",
    "- Has the potential to overwhelm the database.\n",
    "- Requires specification of a stride to properly balance partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columnar Predicate Pushdown\n",
    "\n",
    "The Columnar Predicate Pushdown takes place when a filter can be pushed down to the original data source, such as a database server.\n",
    "\n",
    "In this example, we are going to compare `DataFrames` from two different sources:\n",
    "* JDBC - where a predicate pushdown **WILL** take place.\n",
    "* CSV - where a predicate pushdown will **NOT** take place.\n",
    "\n",
    "In each case, we can see evidence of the pushdown (or lack of it) in the **Physical Plan**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JDBC\n",
    "\n",
    "Start by initializing the JDBC driver.\n",
    "\n",
    "This needs to be done regardless of language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create a `DataFrame` via JDBC and then filter by **gender**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import os\n",
    "import qcutils\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "baseUri = \"s3a://quantia-master/training/\"\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.postgresql:postgresql:42.2.10,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.5 pyspark-shell'\n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "qcutils.init_spark_session(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableName = \"store_sales\"\n",
    "jdbcURL = \"jdbc:postgresql://52.30.211.196/training\"\n",
    "\n",
    "connProperties = {\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"quantia-allianz\"\n",
    "}\n",
    "\n",
    "\n",
    "ppExampleThreeDF = (spark.read.jdbc(\n",
    "    url=jdbcURL,                  # the JDBC URL\n",
    "    table=tableName,              # the name of the table\n",
    "    column=\"ss_sold_date_sk\",     # the name of a column of an integral type that will be used for partitioning\n",
    "    lowerBound=1,                 # the minimum value of columnName used to decide partition stride\n",
    "    upperBound=1000000,           # the maximum value of columnName used to decide partition stride\n",
    "    numPartitions=8,              # the number of partitions/connections\n",
    "    properties=connProperties     # the connection properties\n",
    "  )\n",
    "  .filter(f.col(\"ss_quantity\") > 10)   # Filter the data by quantity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `DataFrame` created, we can ask Spark to `explain(..)` the **Physical Plan**.\n",
    "\n",
    "What we are looking for...\n",
    "* is the lack of a **Filter** and\n",
    "* the presence of a **PushedFilters** in the **Scan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppExampleThreeDF.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will make a little more sense if we **compare it to some examples** that don't push down the filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV File\n",
    "\n",
    "This example is identical to the previous one except...\n",
    "* this is a CSV file instead of JDBC source\n",
    "* we are filtering on **site**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = baseUri + \"wikipedia_pageviews_by_second.tsv\"\n",
    "\n",
    "schema = StructType(\n",
    "  [\n",
    "    StructField(\"timestamp\", StringType(), False),\n",
    "    StructField(\"site\", StringType(), False),\n",
    "    StructField(\"requests\", IntegerType(), False)\n",
    "  ]\n",
    ")\n",
    "\n",
    "ppExampleThreeDF = (spark.read\n",
    "   .option(\"header\", \"true\")\n",
    "   .option(\"sep\", \"\\t\")\n",
    "   .schema(schema)\n",
    "   .csv(csvFile)\n",
    "   .filter(f.col(\"site\") == \"desktop\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `DataFrame` created, we can ask Spark to `explain(..)` the **Physical Plan**.\n",
    "\n",
    "What we are looking for...\n",
    "* is the presence of a **Filter** and\n",
    "* the presence of a **PushedFilters** in the **FileScan csv**\n",
    "\n",
    "And again, we see **PushedFilters** because Spark is *trying* to push down to the CSV file.\n",
    "\n",
    "But that doesn't work here and so we see that just like in the last example, we have a **Filter** after the **FileScan**, actually an **InMemoryFileIndex**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppExampleThreeDF.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ![Quantia Tiny Logo](https://www.quantiaconsulting.com/logos/quantia_logo_tiny.png) 2020 Quantia Consulting, srl. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "Reading Data - Summary",
  "notebookId": 1507370365633369
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
