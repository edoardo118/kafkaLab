{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 2px;\">\n",
    "  <img src=\"https://www.quantiaconsulting.com/logos/quantia_logo_orizz.png\" alt=\"Quantia Consulting\" style=\"width: 600px; height: 250px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Reading Data - JDBC Connections\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "- Read Data from Relational Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Let's start importing libraries and creating useful variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import os\n",
    "import qcutils\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.postgresql:postgresql:42.2.10,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.5 pyspark-shell'\n",
    "\n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "qcutils.init_spark_session(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from JDBC\n",
    "\n",
    "Working with a JDBC data source is significantly different than any of the other data sources.\n",
    "* Configuration settings can be a lot more complex.\n",
    "* Often required to \"register\" the JDBC driver for the target database.\n",
    "* We have to juggle the number of DB connections.\n",
    "* We have to instruct Spark how to partition the data.\n",
    "\n",
    "**NOTE:** The database is read-only\n",
    "* For security reasons. \n",
    "* The notebook does not demonstrate writing to a JDBC database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For examples of writing via JDBC, see \n",
    "  * <a href=\"https://docs.databricks.com/spark/latest/data-sources/sql-databases.html\" target=\"_blank\">Connecting to SQL Databases using JDBC</a>\n",
    "  * <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases\" target=\"_blank\">JDBC To Other Databases</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableName = \"training.people\"\n",
    "\n",
    "jdbcURL = \"jdbc:postgresql://54.195.117.194/training\"\n",
    "\n",
    "connProperties = {\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": \"qcro\",\n",
    "    \"password\": \"qc-readonly\"\n",
    "}\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleOneDF = spark.read.jdbc(\n",
    "    url=jdbcURL,\n",
    "    table=tableName,\n",
    "    properties=connProperties)\n",
    "exampleOneDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleOneDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Compared to CSV and even Parquet, what is missing here?\n",
    "\n",
    "**Question:** Based on the answer to the previous question, what are the ramifications of the missing...?\n",
    "\n",
    "**Question:** Before you run the next cell, what's your best guess as to the number of partitions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Partitions: \" + str(exampleOneDF.rdd.getNumPartitions()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's not Parallelized\n",
    "\n",
    "Let's try this again, and this time we are going to increase the number of connections to the database.\n",
    "\n",
    "**Note:** *If any one of these properties is specified, they must all be specified:*\n",
    "* `partitionColumn` - the name of a column of an integral type that will be used for partitioning.\n",
    "* `lowerBound` - the minimum value of columnName used to decide partition stride.\n",
    "* `upperBound` - the maximum value of columnName used to decide partition stride\n",
    "* `numPartitions` - the number of partitions/connections\n",
    "\n",
    "To quote the <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases\" target=\"_blank\">Spark SQL, DataFrames and Datasets Guide</a>:\n",
    "> These options must all be specified if any of them is specified. They describe how to partition the table when reading in parallel from multiple workers. `partitionColumn` must be a numeric column from the table in question. Notice that `lowerBound` and `upperBound` are just used to decide the partition stride, not for filtering the rows in a table. So all rows in the table will be partitioned and returned. This option applies only to reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleTwoDF = spark.read.jdbc(\n",
    "  url=jdbcURL,                  # the JDBC URL\n",
    "  table=tableName,              # the name of the table\n",
    "  column=\"salary\",     # the name of a column of an integral type that will be used for partitioning.\n",
    "  lowerBound=1,                 # the minimum value of columnName used to decide partition stride.\n",
    "  upperBound=200000,            # the maximum value of columnName used to decide partition stride\n",
    "  numPartitions=8,              # the number of partitions/connections\n",
    "  properties=connProperties)    # the connection properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleTwoDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with checking how many partitions we have (it should be 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Partitions: \" + str(exampleTwoDF.rdd.getNumPartitions()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That might be a problem... notice how many records are in the last partition?\n",
    "\n",
    "**Question:** What are the performance ramifications of leaving our partitions like this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  That's Not [Well] Distributed\n",
    "\n",
    "And this is one of the little gotchas when working with JDBC - to properly specify the stride, we need to know the minimum and maximum value of the IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "minimumID = (exampleTwoDF\n",
    "  .select(min(\"salary\"))   # Compute the minimum ID\n",
    "  .first()[\"min(salary)\"]  # Extract as an integer\n",
    ")\n",
    "maximumID = (exampleTwoDF\n",
    "  .select(max(\"salary\"))   # Compute the maximum ID\n",
    "  .first()[\"max(salary)\"]  # Extract as an integer\n",
    ")\n",
    "print(\"Minimum ID: \" + str(minimumID))\n",
    "print(\"Maximum ID: \" + str(maximumID))\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try this one more time... this time with the proper stride:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleThree = spark.read.jdbc(\n",
    "  url=jdbcURL, # the JDBC URL\n",
    "  table=tableName,                                # the name of the table\n",
    "  column=\"salary\",                       # the name of a column of an integral type that will be used for partitioning.\n",
    "  lowerBound=minimumID,                           # the minimum value of columnName used to decide partition stride.\n",
    "  upperBound=maximumID,                           # the maximum value of columnName used to decide partition stride\n",
    "  numPartitions=8,                                # the number of partitions/connections\n",
    "  properties=connProperties)                      # the connection properties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course we can view that data here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleThree.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ![Quantia Tiny Logo](https://www.quantiaconsulting.com/logos/quantia_logo_tiny.png) Quantia Consulting, srl. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "name": "Reading Data - JDBC",
  "notebookId": 1507370365633248
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
