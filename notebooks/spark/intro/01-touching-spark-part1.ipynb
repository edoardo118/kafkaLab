{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://www.quantiaconsulting.com/logos/quantia_logo_orizz.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 240px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Touching Spark - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Spark Entry Points\n",
    "\n",
    "Since spark 2.x the main entry point for Spark applications is the class `SparkSession`.\n",
    "\n",
    "`SparkSession` gives you access to `Dataframe` and `Dataset` API.\n",
    "\n",
    "`SparkSession` is a replacement for the other entry points:\n",
    "* `SparkContext`, available in our notebook as **sc**.\n",
    "* `SQLContext`, or more specifically it's subclass `HiveContext`, available in our notebook as **sqlContext**.\n",
    "\n",
    "Since Spark 2.0 the usages of `SparkContext` and `SQLContext` are limited (mostly related to `RDD` direct access).\n",
    "\n",
    "It is worth to note that the `SparkContext` is still accessible, but you always need a `SparkSession`!!\n",
    "\n",
    "`SparkSession` function review:\n",
    "* `createDataSet(..)`\n",
    "* `createDataFrame(..)`\n",
    "* `emptyDataSet(..)`\n",
    "* `emptyDataFrame(..)`\n",
    "* `range(..)`\n",
    "* `read(..)`\n",
    "* `readStream(..)`\n",
    "* `sparkContext(..)`\n",
    "* `sqlContext(..)`\n",
    "* `sql(..)`\n",
    "* `streams(..)`\n",
    "* `table(..)`\n",
    "* `udf(..)`\n",
    "\n",
    "In the next sections, the function we are most interested in is `SparkSession.sparkContext()` which returns a `SparkContext`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Let's start Creating SparkSession and useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import qcutils\n",
    "from pyspark.sql import SparkSession\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "baseUri = \"s3a://quantia-master/training/\"\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.5 pyspark-shell'\n",
    "\n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "qcutils.init_spark_session(spark)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start from the bottom, the `RDD`.\n",
    "\n",
    "We need the `SparkContext`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png)  RDD Overview\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "* Have a first touch with RDD\n",
    "* Perform a simple task\n",
    "  * Read a text file\n",
    "  * perform a word count operation\n",
    "  \n",
    "Let's take a look on the text file we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcutils.print_s3_bucket_object(key='training/word-count-small.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to create an RDD from the file and perform a count operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Open textFile for Spark Context RDD\n",
    "text_file = sc.textFile(baseUri + \"word-count-small.txt\")\n",
    "\n",
    "# Execute word count\n",
    "counts = text_file.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png)  Transformations & Actions\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "* Review the Lazy vs. Eager design\n",
    "* Quick review of Transformations\n",
    "* Quick review of Actions\n",
    "* Introduce the Catalyst Optimizer\n",
    "* Wide vs. Narrow Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laziness By Design\n",
    "\n",
    "RDDs support two types of operations: \n",
    "* Transformations, which create a new dataset from an existing one, and \n",
    "* Actions, which return a value to the driver program after running a computation on the dataset. \n",
    "\n",
    "For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).\n",
    "\n",
    "Fundamental to Apache Spark are the notions that:\n",
    "* Transformations are **LAZY**: they do not compute their results immediately, but the system just remember the transformations applied to some base dataset and applies them once needed\n",
    "* Actions are **EAGER**: they are computed immediatly, together with all the previous (needed) transformations.\n",
    "\n",
    "This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n",
    "\n",
    "We see this play out when we run multiple transformations back-to-back, and no job is triggered:\n",
    "\n",
    "#### Spark UI - Jobs\n",
    "\n",
    "In the **Spark UI**'s **Jobs** page it is available a detailed list of the spark jobs.\n",
    "\n",
    "The jobs list in the Spark UI is empty..... Why?\n",
    "\n",
    "....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only run **transformations**!!!\n",
    "\n",
    "Let's now try with an **action**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the results\n",
    "output = counts.collect()\n",
    "\n",
    "# Print the first 10 elements of the output array\n",
    "for (word, count) in output[:10]:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a single job is listed, a `collect` job\n",
    "\n",
    "Note that the `collect` function of the the `RDD`, returns (materializes in the driver) an array that contains all of the elements in this RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Little Step More\n",
    "\n",
    "#### Wide vs. Narrow Transformations\n",
    "\n",
    "Transformations can be classified into two broad categories: **wide** and **narrow**.\n",
    "\n",
    "**Narrow Transformations**: The data required to compute the records in a single partition reside in at most one partition of the parent RDD.\n",
    "\n",
    "Examples include:\n",
    "* `filter(..)`\n",
    "* `map(..)`\n",
    "* `...`\n",
    "\n",
    "<img src=\"https://www.quantiaconsulting.com/logos/img/transformations-narrow.png\" alt=\"Narrow Transformations\" style=\"height:300px\"/>\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Wide Transformations**: The data required to compute the records in a single partition may reside in many partitions of the parent RDD. \n",
    "\n",
    "Examples include:\n",
    "* `groupBy(...).sum()` \n",
    "* `distinct()` \n",
    "* `...` \n",
    "\n",
    "<img src=\"https://www.quantiaconsulting.com/logos/img/transformations-wide.png\" alt=\"Wide Transformations\" style=\"height:300px\"/>\n",
    "\n",
    "#### Shuffle\n",
    "\n",
    "What if the data you need is not on the same executor and you want to optimize your execution?...\n",
    "\n",
    "You need to `shuffle` data!\n",
    "\n",
    "Remember the previuos image, What if you need to to group by color, it will serve us best if...\n",
    "  * All the reds are in one partitions\n",
    "  * All the blues are in a second partition\n",
    "  * All the greens are in a third\n",
    "\n",
    "From there we can easily sum/count/average all of the reds, blues, and greens.\n",
    "\n",
    "To carry out the shuffle operation Spark needs to\n",
    "* Write that data to disk on the local node - at this point the slot is free for the next task.\n",
    "* Send that data across the wire to another executor\n",
    "  * Technically the Driver decides which executor gets which piece of data.\n",
    "  * Then the executor pulls the data it needs from the other executor's shuffle files.\n",
    "* Copy the data back into RAM on the new executor\n",
    "  * The concept, if not the action, is just like the initial read \"every\" `DataFrame` starts with.\n",
    "\n",
    "**Note:** Some actions, like `count()` and `reduce(..)`, needs a shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Caching\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "* Understand how caching works\n",
    "* Explore the different caching mechanisims\n",
    "* Discuss tips for the best use of the cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can **persistst** (or **caching**) a dataset in memory across operations.\n",
    "\n",
    "Persist an RDD means that each node stores any computed partitions (of the persisted RDD) in memory in order to speed-up the reuse of that partition in in other actions on that dataset (or datasets derived from it). \n",
    "\n",
    "Caching plays a key role in improving iterative algorithms performance. A good usage of cache operation can speed-up operations by more than 10x.\n",
    "\n",
    "Note that an RDD can be marked to be cached (using [persist()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#persist--) or [cache()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#cache--) methods). The caching operation is lazy, so the RDD will be cached the first time it is computed in an action.\n",
    "\n",
    "Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark UI - Storage\n",
    "\n",
    "In the **Spark UI**'s **Storage** page it is available a detailed list of the cached RDDs.\n",
    "\n",
    "Let's review fields:\n",
    "* RDD Name\n",
    "* Storage Level\n",
    "* Cached Partitions\n",
    "* Fraction Cached\n",
    "* Size in Memory\n",
    "* Size on Disk\n",
    "\n",
    "Let's play with cache.\n",
    "\n",
    "Count the number of words in the file `...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcutils.print_s3_bucket_object(key='training/enwiki-latest-abstract10.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open textFile for Spark Context RDD\n",
    "text_file = sc.textFile(baseUri + \"enwiki-latest-abstract10.xml\")\n",
    "\n",
    "# Execute word count\n",
    "counts = text_file.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
    "output = counts.collect()\n",
    "\n",
    "# Print the first 10 elements of the output array\n",
    "for (word, count) in output[:10]:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".... now cache the file before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the spark UI... no entry in the **storage** page... Why?\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** the `count()` action **materializes the cache**\n",
    "\n",
    "The `cache()` is neither an Action nor a Transformation, we can mark an RDD as cachable, Spark will decide if and when cache it.\n",
    "\n",
    "We can't force caching every time, in this case the `count()` works well because we are running in educational environment, with very few data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = text_file.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
    "output = counts.collect()\n",
    "\n",
    "# Print the first 10 elements of the output array\n",
    "for (word, count) in output[:10]:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Little Step More\n",
    "\n",
    "#### Storage Level\n",
    "\n",
    "Based on your system characteristics or operation needs, each persisted RDD can be stored using a different storage level:\n",
    "\n",
    "* `MEMORY_ONLY`: Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. This is the default level.\n",
    "* `MEMORY_AND_DISK`: Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed.\n",
    "* `MEMORY_ONLY_SER` (Java and Scala): Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.\n",
    "* `MEMORY_AND_DISK_SER` (Java and Scala): Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed.\n",
    "* `DISK_ONLY`: Store the RDD partitions only on disk.\n",
    "* `MEMORY_ONLY_2`, `MEMORY_AND_DISK_2`, etc.: Same as the levels above, but replicate each partition on two cluster nodes.\n",
    "* `OFF_HEAP`: Similar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to be enabled.\n",
    "\n",
    "**Note** The storage level can be passed to `persist()` method, the `cache()` method is a shorthand for using the default storage level, which is StorageLevel.MEMORY_ONLY (store deserialized objects in memory)\n",
    "\n",
    "\n",
    "#### Which Storage Level to Choose?\n",
    "\n",
    "Different storage levels offer different memory usage/CPU efficiency trade-offs. But how to choose?\n",
    "\n",
    "The following guidelines could help:\n",
    "* If you have enough memory (RAM) and the RDDs fit comfortably, use `MEMORY_ONLY` storage level. The `MEMORY_ONLY` level is the most CPU-efficient option, allowing operations on the RDDs to run as fast as possible.\n",
    "* Don’t spill to disk unless the functions that computed your datasets are expensive, or they filter a large amount of the data. Otherwise, recomputing a partition may be as fast as reading it from disk.\n",
    "* Use the replicated storage levels if you want fast fault recovery (e.g. if using Spark to serve requests from a web application). All the storage levels provide full fault tolerance by recomputing lost data, but the replicated ones let you continue running tasks on the RDD without waiting to recompute a lost partition.\n",
    "\n",
    "|        Level        \t| Space used \t| CPU time \t| In memory \t| On disk \t| Serialized \t|\n",
    "|:-------------------:\t|:----------:\t|:--------:\t|:---------:\t|:-------:\t|:----------:\t|\n",
    "| MEMORY_ONLY         \t|    High    \t|    Low   \t|     Y     \t|    N    \t|      N     \t|\n",
    "| MEMORY_ONLY_SER     \t|     Low    \t|   High   \t|     Y     \t|    N    \t|      Y     \t|\n",
    "| MEMORY_AND_DISK     \t|    High    \t|  Medium  \t|    Some   \t|   Some  \t|    Some    \t|\n",
    "| MEMORY_AND_DISK_SER \t|     Low    \t|   High   \t|    Some   \t|   Some  \t|      Y     \t|\n",
    "| DISK_ONLY           \t|     Low    \t|   High   \t|     N     \t|    Y    \t|      Y     \t|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep dive in Storage Level\n",
    "For the next section, we need to clear the existing cache using the `unpersist()` method on every single cached RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go deeper in the storage level.\n",
    "\n",
    "As a first step, we need to clean the cache!\n",
    "\n",
    "In order to upersist an RDD, we need to use the `unpersist()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now read the file in three different RDDs the same file and persist two of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcutils.print_s3_bucket_object(key='training/enwiki-latest-abstract.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "text_file_1 = sc.textFile(baseUri + \"enwiki-latest-abstract.xml\")\n",
    "text_file_2 = sc.textFile(baseUri + \"enwiki-latest-abstract.xml\")\n",
    "text_file_3 = sc.textFile(baseUri + \"enwiki-latest-abstract.xml\")\n",
    "\n",
    "text_file_2.persist(StorageLevel.MEMORY_ONLY).count()\n",
    "text_file_3.persist(StorageLevel.DISK_ONLY).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Spark UI storage tab we can see the difference between the cached RDD!\n",
    "\n",
    "Now, let's try to perform trasformations and actions on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_1.filter(lambda s: \"italy\" in s).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_2.filter(lambda s: \"italy\" in s).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_3.filter(lambda s: \"italy\" in s).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between the performace of the operations on the 3 files?\n",
    "\n",
    "* `text_file_1` is not cached and every action implies a new read of the original file from the remote storage\n",
    "* `text_file_2` is only partially cached and every action implies a partial read of the original file from the remote storage -> It leads to a partial perfromance improvement\n",
    "* `text_file_3` is completely cached on local disk and the performance results the best\n",
    "\n",
    "**Note** Since spark 2.x the default level of storage (the one used by `cache()`) is `MEMORY_AND_DISK`. \n",
    "Using the `MEMORY_AND_DISK` storage level, the non-cached partition must be recomputed every time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the more you use the cache, the better it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_2.filter(lambda s: \"europe\" in s).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png)  Take Home Messages\n",
    "\n",
    "* RDDs are the main brick of Spark application\n",
    "* RDD are distributed and immutable\n",
    "* Spark is **Lazy** (...not always!)\n",
    "* Caching improve performance, but cache must be used in a smart way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Let's Get Dirty\n",
    "\n",
    "Let's put at work what we've just learnt on RDD with a [Lab](./lab/01a-touching-spark-part1-lab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ![Quantia Tiny Logo](https://www.quantiaconsulting.com/logos/quantia_logo_tiny.png) 2020 Quantia Consulting, srl. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "spark-hands-on",
  "notebookId": 4377518618624333
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
