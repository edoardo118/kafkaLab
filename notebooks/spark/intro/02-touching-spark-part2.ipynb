{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://www.quantiaconsulting.com/logos/quantia_logo_orizz.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 240px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Touching Spark - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Let's start again by creating a new SparkSession.\n",
    "\n",
    "Note that we are working in a notebook environment, each notebook runs its own spark session..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import qcutils\n",
    "from pyspark.sql import SparkSession\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "baseUri = \"s3a://quantia-master/training/\"\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.5 pyspark-shell'\n",
    "\n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "qcutils.init_spark_session(spark)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Dataset and DataDrame Overview\n",
    "\n",
    "Dataset and Dataframe are built on top of RDDs and represent an higher level abstraction.\n",
    "\n",
    "A Dataset is an immutable distributed collection of data. It is available since Spark 1.6 and offers the benefits of RDDs (strong typing and lambda functions) with the benefits of Spark SQLâ€™s optimized execution engine. \n",
    "The Dataset API is **only** available in Scala and Java. \n",
    "\n",
    "`DataFrame` is a type alias of `Dataset[Row]` (from Java/Scala perspective). It is organized into named columns and conceptually equivalent to a table in a relational database or a data frame in R/Python.\n",
    "The DataFrame API is available in Scala, Java, Python, and R.\n",
    "\n",
    "Since spark 2.x DataFrame APIs will merge with Datasets APIs, unifying data processing capabilities across libraries.\n",
    "\n",
    "### Dataset/Dataframe API Benefits\n",
    "* Type-safety at runtime and Strong-typing: syntax error at compile time vs syntax error at runtime\n",
    "<img src=https://www.quantiaconsulting.com/logos/img/sql-vs-dataframes-vs-datasets-type-safety-spectrum.png width=\"450\">\n",
    "\n",
    "<br/>\n",
    "\n",
    "* High-level abstraction of structured and semi-structured data:Dataset/Dataframe APIs create a structured view of your semi-structured data (JSON source -> Dataframe Object)\n",
    "\n",
    "* Easy-to-use APIs: `agg`, `select`, `sum`, `filter`, or `groupBy` vs `map`, `flatMap` or `reduceByKey`\n",
    "\n",
    "* Performance and Optimization:\n",
    "<img src=https://www.quantiaconsulting.com/logos/img/memory-usage-when-caching-datasets-vs-rdds.png width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section uses [**Pageviews By Seconds** data set](https://dumps.wikimedia.org/other/pagecounts-raw/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Data Ingestion\n",
    "\n",
    "The first basic operation during a data project is ingest data.\n",
    "\n",
    "The SparkSession entrypoint offers the `read` function to directly create Dataframes from external sources.\n",
    "\n",
    "Spark offers readers for all of the most common structured data:\n",
    "* Structured Text file (csv, tsv, etc)\n",
    "* Big data format (parquet, orc, etc)\n",
    "* Semistructured data (json)\n",
    "* Relational source (JDBC)\n",
    "* ...\n",
    "\n",
    "Most of this reader offers option to automate basic step during ingestion.\n",
    "\n",
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "  [\n",
    "    StructField(\"timestamp\", StringType(), False),\n",
    "    StructField(\"site\", StringType(), False),\n",
    "    StructField(\"requests\", IntegerType(), False)\n",
    "  ]\n",
    ")\n",
    "\n",
    "fileName = baseUri + \"wikipedia_pageviews_by_second.tsv\"\n",
    "\n",
    "pageviewsDF = (spark.read\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .schema(schema)\n",
    "  .csv(fileName)\n",
    ")\n",
    "\n",
    "pageviewsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in the previous read operation we tell the reader:\n",
    "* That the separator is `\\t` -> `.option(\"sep\", \"\\t\")`\n",
    "* The file has an header -> `.option(\"header\", \"true\")`\n",
    "* The schema for the columns is the one we described above -> `.schema(schema)`\n",
    "    * we could ask the system to guess the schema -> `.option(\"inferSchema\", \"true\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Transformations & Actions (From DataFrame Perspective)\n",
    "\n",
    "### Transformations\n",
    "\n",
    "Transformations have the following key characteristics:\n",
    "* always return a `DataFrame` (or a `DataSet[Row]` the case of Java/Scala).\n",
    "* are immutable - a `DataFrame` instance of cannot be altered once it's instantiated (further optimizations are still possible)\n",
    "* are classified as either a Wide or Narrow operation\n",
    "* in **Scala & Java** come in two flavors: Typed & Untyped\n",
    "\n",
    "### Actions\n",
    "\n",
    "In contrast to transformation, Actions either return a result or write to disc. For example:\n",
    "* The number of records in the case of `count()` \n",
    "* An array of objects in the case of `collect()` or `take(n)`\n",
    "\n",
    "Hereafter you can find a list of the most important Actions\n",
    "\n",
    "| Method | Return | Description |\n",
    "|--------|--------|-------------|\n",
    "| `collect()` | Collection | Returns an array that contains all of Rows in this DataFrame. |\n",
    "| `count()` | Long | Returns the number of rows in the DataFrame. |\n",
    "| `first()` | Row | Returns the first row. |\n",
    "| `foreachPartition(f)` | - | Applies a function f to each partition of this DataFrame. |\n",
    "| `head()` | Row | Returns the first row. |\n",
    "| `show(..)` | - | Displays the top 20 rows of Dataset in a tabular form. |\n",
    "| `take(n)` | Collection | Returns the first n rows in the DataFrame. |\n",
    "| `toLocalIterator()` | Iterator | Return an iterator that contains all of Rows in this DataFrame. |\n",
    "| `...`||\n",
    "\n",
    "\n",
    "**Note:** The list of transformations and actions varies significantly between each language. Mostly because Java & Scala are strictly typed languages compared Python & R which are loosed typed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now put at work some transformations and actions.\n",
    "\n",
    "The 255 MB pageviews file is currently in our object store, which means each time you scan through it, your Spark cluster has to read the 255 MB of data remotely over the network.\n",
    "\n",
    "This time let's try to perform and `filter` transformation and a `count` to trigger jobs.\n",
    "\n",
    "Once again, use the `count()` action to scan the entire 255 MB file from disk and count how many total records (rows) there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpv = pageviewsDF.filter('site==\"mobile\"')\n",
    "fpv.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the cell several times and take note of an average execution time.\n",
    "\n",
    "Every time we re-run these operations, it goes all the way back to the original data store.\n",
    "\n",
    "This requires pulling all the data across the network for every execution.\n",
    "\n",
    "In many/most cases, this network IO is the most expensive part of a job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cache()\n",
    "\n",
    "We limit the overhead by caching the data on the executors.\n",
    "\n",
    "As for an RDD, the `cache(..)` operation doesn't do anything other than mark a `DataFrame` as cacheable.\n",
    "\n",
    "It is not technically a transformation or action, and, in order to actually cache the data, Spark has to process over every single record.\n",
    "\n",
    "A very common method for materializing the cache is to execute a `count()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageviewsDF.cache()\n",
    "pageviewsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last `count()` will take a little longer than normal.\n",
    "\n",
    "It has to perform the cache and do the work of materializing the cache.\n",
    "\n",
    "Now that `pageviewsDF` is cached **AND** the cache has been materialized.\n",
    "\n",
    "Now, run the two queries and compare their execution time to the ones above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpv = pageviewsDF.filter('site==\"mobile\"')\n",
    "fpv.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster, right?\n",
    "\n",
    "We are no longer making network calls, now all of our data is being stored in RAM on the executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use directly SQL!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageviewsDF.createOrReplaceTempView(\"pageviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT count(*) AS n \n",
    "FROM pageviews \n",
    "WHERE site==\"mobile\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Differently from RDDs, there are several ways to clean cache:\n",
    "  * Remove each cache one-by-one, fairly problematic -> `unpersist()`\n",
    "  * Restart the cluster - takes a fair while to come back online\n",
    "  * Just blow the entire cache away - this will affect every user on the cluster!! -> `spark.catalog.clearCache()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Little Step More\n",
    "\n",
    "#### Why is Laziness So Important?\n",
    "\n",
    "As for RDD, Spark is Lazy for DataFrame Transformations too. But why?\n",
    "\n",
    "The laziness a common pattern in functional programming.\n",
    "It has a number of benefits\n",
    "* Not forced to load all data at step #1 \n",
    "  * Technically impossible with **REALLY** large datasets.\n",
    "* Easier to parallelize operations \n",
    "  * N different transformations can be processed on a single data element, on a single thread, on a single machine. \n",
    "* Most importantly, it allows the framework to **automatically apply various optimizations**\n",
    "\n",
    "We will see it at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Data Preparation \n",
    "\n",
    "As you can see, DataFrame ease the data ingestion and the data exploration.\n",
    "\n",
    "Moreover, the tabular data structure makes easy to manipulate and change the original format of the data.\n",
    "\n",
    "**Note** As for RDD, a DataFrame is immutable once instatiated. In order to change the schema or the content in a persistent way, you have to re-assign the DataFrame.\n",
    "\n",
    "Examples of most common data manipulations are:\n",
    "* Change the data structure by adding/renaming/deleting columns\n",
    "* Manipulate date and time\n",
    "\n",
    "Let's play with more complex data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeSharingDF = (spark.read\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(baseUri+\"bikeSharing.csv\"))\n",
    "\n",
    "bikeSharingDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeSharingDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are seeing here?\n",
    "\n",
    "A pretty large file, with 17 columns and thousands of rows.\n",
    "\n",
    "This is an open source dataset that contains the number of active bike per hour, together with accurated information related to the date, the weather and the season.\n",
    "\n",
    "Based on your needs, it can be useful to add/remove columns or filter rows or change the data type of a column.\n",
    "\n",
    "Let's start taking a look to the column name and data-type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeSharingDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Columns and Column Name\n",
    "\n",
    "The name of a column represents and important starting point to understand data.\n",
    "\n",
    "The method `withColumnRenamed(...)` change the name of an existing column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeSharingDF.withColumnRenamed(\"hr\", \"hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just changed the name of `hr` column to `hour`, but.... remember that DataFrame is immutable! You have to re-assign it.\n",
    "\n",
    "You can re-assign it to the same variable, but you will lose the reference to the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeSharingDF = bikeSharingDF.withColumnRenamed(\"hr\", \"hour\")\n",
    "bikeSharingDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataType\n",
    "\n",
    "Spark did a great job during the ingestion in inferring the right datatype, but what if we need to change the datatype of a column?\n",
    "\n",
    "We can perform a `cast`. \n",
    "\n",
    "A cast can change the datatype of a column.\n",
    "\n",
    "Let's start from a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeSharingDF = bikeSharingDF.withColumn(\"instant\", col(\"instant\").cast(StringType()))\n",
    "bikeSharingDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the instant column is of type `String`.\n",
    "\n",
    "We used the `withColumn()` method, that let you add a new column to a Dataframe specifying a function to apply to the data.\n",
    "\n",
    "**Note** We used an existing name in the `withColumn()`, the new column replace the old one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Little Step More\n",
    "\n",
    "#### DateTime Manipulation\n",
    "\n",
    "The date and time manipulation represents a very common step in a data preparation pipeline.\n",
    "\n",
    "Let's start with an example, in our DataFrame, we want create a single datetime column with the information related to the day (now in `dteday`) column and related to the hour (now in the column `hour`), how can we do that?\n",
    "\n",
    "Spark offers a wide range of function to manipulate date and time.\n",
    "\n",
    "At this moment we have:\n",
    "* `dteday`: timestamp column\n",
    "* `hour`: integer column\n",
    "\n",
    "Let's start creating a new column and trying to merge the two information.\n",
    "\n",
    "Any suggestion?\n",
    "\n",
    "**HINT** Unix timestamp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeSharingDF.withColumn(\"dt\", from_unixtime(unix_timestamp('dteday') + col(\"hour\") * 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Catalyst Optimizer\n",
    "\n",
    "![Catalyst](https://www.quantiaconsulting.com/logos/img/catalyst-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Logical Plan\n",
    "\n",
    "The Catalyst Optimizer **rewrites our code**\n",
    "\n",
    "In the next section we will see **two examples** involving the rewriting of our filters.\n",
    "\n",
    "The first is an **innocent mistake** almost most every new Spark developer makes.\n",
    "\n",
    "The second \"mistake\" is... well... **really bad** - but Spark can fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example #1: Innocent Mistake\n",
    "\n",
    "I don't want any project that starts with **en.zero**.\n",
    "\n",
    "There are **better ways of doing this**, as in it can be done with a single condition.\n",
    "\n",
    "But we will make **8 passes** on the data **with 8 different filters**.\n",
    "\n",
    "After every individual pass, we will **go back over the remaining dataset** to filter out the next set of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = baseUri + \"wikipedia_pagecount.parquet\"\n",
    "\n",
    "allDF = spark.read.parquet(parquetFile)\n",
    "\n",
    "pass1 = allDF.filter( col(\"project\") != \"en.zero\")\n",
    "pass2 = pass1.filter( col(\"project\") != \"en.zero.n\")\n",
    "pass3 = pass2.filter( col(\"project\") != \"en.zero.s\")\n",
    "pass4 = pass3.filter( col(\"project\") != \"en.zero.d\")\n",
    "pass5 = pass4.filter( col(\"project\") != \"en.zero.voy\")\n",
    "pass6 = pass5.filter( col(\"project\") != \"en.zero.b\")\n",
    "pass7 = pass6.filter( col(\"project\") != \"en.zero.v\")\n",
    "pass8 = pass7.filter( col(\"project\") != \"en.zero.q\")\n",
    "\n",
    "print(\"Pass 1: {0:,}\".format( pass1.count() ))\n",
    "print(\"Pass 2: {0:,}\".format( pass2.count() ))\n",
    "print(\"Pass 3: {0:,}\".format( pass3.count() ))\n",
    "print(\"Pass 4: {0:,}\".format( pass4.count() ))\n",
    "print(\"Pass 5: {0:,}\".format( pass5.count() ))\n",
    "print(\"Pass 6: {0:,}\".format( pass6.count() ))\n",
    "print(\"Pass 7: {0:,}\".format( pass7.count() ))\n",
    "print(\"Pass 8: {0:,}\".format( pass8.count() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logically**, the code above is the same as the code below.\n",
    "\n",
    "The only real difference is that we are **not asking for a count** after every filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innocentDF = (spark.read.parquet(parquetFile)\n",
    "  .filter( col(\"project\") != \"en.zero\")\n",
    "  .filter( col(\"project\") != \"en.zero.n\")\n",
    "  .filter( col(\"project\") != \"en.zero.s\")\n",
    "  .filter( col(\"project\") != \"en.zero.d\")\n",
    "  .filter( col(\"project\") != \"en.zero.voy\")\n",
    "  .filter( col(\"project\") != \"en.zero.b\")\n",
    "  .filter( col(\"project\") != \"en.zero.v\")\n",
    "  .filter( col(\"project\") != \"en.zero.q\")\n",
    ")\n",
    "print(\"Final Count: {0:,}\".format( innocentDF.count() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't even have to execute the code to see what is **logically** or **physically** taking place under the hood.\n",
    "\n",
    "Here we can use the `explain(..)` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innocentDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if we were to write this the correct way, the first time, ignoring the fact that there are better methods, it would look something like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betterDF = (spark.read.parquet(parquetFile)\n",
    "  .filter( (col(\"project\").isNotNull()) &\n",
    "           (col(\"project\") != \"en.zero\") & \n",
    "           (col(\"project\") != \"en.zero.n\") & \n",
    "           (col(\"project\") != \"en.zero.s\") & \n",
    "           (col(\"project\") != \"en.zero.d\") & \n",
    "           (col(\"project\") != \"en.zero.voy\") & \n",
    "           (col(\"project\") != \"en.zero.b\") & \n",
    "           (col(\"project\") != \"en.zero.v\") & \n",
    "           (col(\"project\") != \"en.zero.q\")\n",
    "        )\n",
    ")\n",
    "\n",
    "print(\"Final: {0:,}\".format( betterDF.count() ))\n",
    "\n",
    "betterDF.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = baseUri + \"wikipedia_pagecount.parquet\"\n",
    "\n",
    "allDF = spark.read.parquet(parquetFile)\n",
    "\n",
    "pass1 = allDF.filter( col(\"project\") != \"en.zero\")\n",
    "pass2 = pass1.filter( col(\"project\") != \"en.zero.n\")\n",
    "pass3 = pass2.filter( col(\"project\") != \"en.zero.s\")\n",
    "pass4 = pass3.filter( col(\"project\") != \"en.zero.d\")\n",
    "pass5 = pass4.filter( col(\"project\") != \"en.zero.voy\")\n",
    "pass6 = pass5.filter( col(\"project\") != \"en.zero.b\")\n",
    "pass7 = pass6.filter( col(\"project\") != \"en.zero.v\")\n",
    "pass8 = pass7.filter( col(\"project\") != \"en.zero.q\")\n",
    "\n",
    "print(\"Final: {0:,}\".format( pass8.count() ))\n",
    "\n",
    "pass8.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example #2: Bad Programmer\n",
    "\n",
    "This time we are going to do something **REALLY** bad...\n",
    "\n",
    "Even if the compiler combines these filters into a single filter, **we still have five different tests** for any column that doesn't have the value \"whatever\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stupidDF = (spark.read.parquet(parquetFile)\n",
    "  .filter( col(\"project\") != \"whatever\")\n",
    "  .filter( col(\"project\") != \"whatever\")\n",
    "  .filter( col(\"project\") != \"whatever\")\n",
    "  .filter( col(\"project\") != \"whatever\")\n",
    "  .filter( col(\"project\") != \"whatever\")\n",
    ")\n",
    "\n",
    "stupidDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** `explain(..)` is not the only way to get access to this level of detail...\n",
    "We can also see it in the **Spark UI**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png)  Take Home Messages\n",
    "\n",
    "* DataFrame API make Spark \"accessible\" and reduce complexity of Spark application\n",
    "* Caching DataFrame \n",
    "* Laziness is important for optimization (especially in a complex pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Let's Get Dirty\n",
    "\n",
    "Let's put at work what we've just learnt on DataFrame with [Lab2a](./lab/02a-touching-spark-part2-lab.ipynb) and [Lab2b](./lab/02b-touching-spark-part2-lab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ![Quantia Tiny Logo](https://www.quantiaconsulting.com/logos/quantia_logo_tiny.png) 2020 Quantia Consulting, srl. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "spark-hands-on",
  "notebookId": 4377518618624333
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
