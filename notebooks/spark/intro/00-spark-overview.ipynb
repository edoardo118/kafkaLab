{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://www.quantiaconsulting.com/logos/quantia_logo_orizz.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 240px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Spark Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Short History of Apache Spark\n",
    "* <a href=\"https://en.wikipedia.org/wiki/Apache_Spark\" target=\"_blank\">Apache Spark</a> started as a research project at the \n",
    "University of California AMPLab, in 2009 by <a href=\"https://en.wikipedia.org/wiki/Matei_Zaharia\" target=\"_blank\">Matei Zaharia</a>.\n",
    "* In 2013, the project was\n",
    "  * donated to the Apache Software Foundation\n",
    "  * open sourced\n",
    "  * adopted the Apache 2.0 license\n",
    "* In February 2014, Spark became a Top-Level <a href=\"https://spark.apache.org/\" target=\"_blank\">Apache Project<a/>.\n",
    "* Latest stable release: <a href=\"https://spark.apache.org/downloads.html\" target=\"_blank\">CLICK-HERE</a>\n",
    "* 600,000+ lines of code (75% Scala)\n",
    "* Built by 1,000+ developers from more than 250+ organizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) What is Apache Spark?\n",
    "\n",
    "Spark is a unified processing engine that can analyze big data using SQL, machine learning, graph processing or real-time stream analysis:\n",
    "\n",
    "![Spark Engines](https://www.quantiaconsulting.com/logos/img/spark_4engines.png)\n",
    "<br/>\n",
    "<br/>\n",
    "* At its core is the Spark Engine.\n",
    "* The DataFrames API provides an abstraction above RDDs while simultaneously improving performance 5-20x over traditional RDDs with its Catalyst Optimizer.\n",
    "* Spark ML provides high quality and finely tuned machine learning algorithms for processing big data.\n",
    "* The Graph processing API gives us an easily approachable API for modeling pairwise relationships between people, objects, or nodes in a network.\n",
    "* The Streaming APIs give us End-to-End Fault Tolerance, with Exactly-Once semantics, and the possibility for sub-millisecond latency.\n",
    "\n",
    "And it all works together seamlessly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) A Unifying Engine\n",
    "\n",
    "And as a compute engine, Apache Spark is not tied to a specific environment or data warehouse strategy.\n",
    "\n",
    "![Unified Engine](https://www.quantiaconsulting.com/logos/img/unified-engine.png)\n",
    "<br/>\n",
    "<br/>\n",
    "* Built upon the Spark Core\n",
    "* Apache Spark is data and environment agnostic.\n",
    "* Languages: **Scala, Java, Python, R, SQL**\n",
    "* Environments: **Yarn, Docker, EC2, Mesos, OpenStack, Databricks (our favorite), Digital Ocean, and much more...**\n",
    "* Data Sources: **Hadoop HDFS, Casandra, Kafka, Apache Hive, HBase, JDBC (PostgreSQL, MySQL, etc.), CSV, JSON, Azure Blob, Amazon S3, ElasticSearch, Parquet, and much, much more...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) RDDs\n",
    "* The primary data abstraction of Spark engine is the RDD: Resilient Distributed Dataset\n",
    "  * Resilient, i.e., fault-tolerant with the help of RDD lineage graph and so able to recompute missing or damaged partitions due to node failures.\n",
    "  * Distributed with data residing on multiple nodes in a cluster.\n",
    "  * Dataset is a collection of partitioned data with primitive values or values of values, e.g., tuples or other objects.\n",
    "* The original paper that gave birth to the concept of RDD is <a href=\"https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf\" target=\"_blank\">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</a> by Matei Zaharia et al.\n",
    "* Since Spark 2.x, RDDs are considered as the assembly language of the Spark ecosystem.\n",
    "* DataFrames, Datasets & SQL provide the higher level abstraction over RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Scala, Python, Java, R & SQL\n",
    "* Besides being able to run in many environments...\n",
    "* Apache Spark makes the platform even more approachable by supporting multiple languages:\n",
    "  * Scala - Apache Spark's primary language.\n",
    "  * Python - More commonly referred to as PySpark\n",
    "  * R - <a href=\"https://spark.apache.org/docs/latest/sparkr.html\" target=\"_blank\">SparkR</a> (R on Spark)\n",
    "  * Java\n",
    "  * SQL - Closer to ANSI SQL 2003 compliance\n",
    "    * Since spark 2.x running all 99 TPC-DS queries\n",
    "    * New standards-compliant parser (with good error messages!)\n",
    "    * Subqueries (correlated & uncorrelated)\n",
    "    * Approximate aggregate stats\n",
    "* With the older RDD API, there are significant differences with each language's implementation, namely in performance.\n",
    "* With the newer DataFrames API, the performance differences between languages are nearly nonexistence (especially for Scala, Java & Python).\n",
    "* With that, not all languages get the same amount of love - just the same, that API gap for each language is rapidly closing, especially between Spark 1.x and 2.x.\n",
    "\n",
    "![RDD vs DataFrames](https://www.quantiaconsulting.com/logos/img/rdd-vs-dataframes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) The Cluster: Drivers, Executors, Slots & Tasks\n",
    "![Spark Physical Cluster, slots](https://www.quantiaconsulting.com/logos/img/spark_cluster_slots.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **Driver** is the JVM in which our application runs.\n",
    "* The secret to Spark's awesome performance is parallelism.\n",
    "  * Scaling vertically is limited to a finite amount of RAM, Threads and CPU speeds.\n",
    "  * Scaling horizontally means we can simply add new \"nodes\" to the cluster almost endlessly.\n",
    "* We parallelize at two levels:\n",
    "  * The first level of parallelization is the **Executor** - a Java virtual machine running on a node, typically, one instance per node.\n",
    "  * The second level of parallelization is the **Slot** - the number of which is determined by the number of cores and CPUs of each node.\n",
    "* Each **Executor** has a number of **Slots** to which parallelized **Tasks** can be assigned to it by the **Driver**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Physical Cluster, tasks](https://files.training.databricks.com/images/105/spark_cluster_tasks.png)\n",
    "<br/>\n",
    "<br/>\n",
    "* The JVM is naturally multithreaded, but a single JVM, such as our **Driver**, has a finite upper limit.\n",
    "* By creating **Tasks**, the **Driver** can assign units of work to **Slots** for parallel execution.\n",
    "* Additionally, the **Driver** must also decide how to partition the data so that it can be distributed for parallel processing (not shown here).\n",
    "* Consequently, the **Driver** is assigning a **Partition** of data to each task - in this way each **Task** knows which piece of data it is to process.\n",
    "* Once started, each **Task** will fetch from the original data source the **Partition** of data assigned to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Quick Note on Jobs & Stages\n",
    "* Each parallelized action is referred to as a **Job**.\n",
    "* The results of each **Job** (parallelized/distributed action) is returned to the **Driver**.\n",
    "* Depending on the work required, multiple **Jobs** will be required.\n",
    "* Each **Job** is broken down into **Stages**. \n",
    "* This would be analogous to building a house (the job)\n",
    "  * The first stage would be to lay the foundation.\n",
    "  * The second stage would be to erect the walls.\n",
    "  * The third stage would be to add the room.\n",
    "  * Attempting to do any of these steps out of order just won't make sense, if not just impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Cluster Management & Local Mode\n",
    "\n",
    "* At a much lower level, Spark Core employs a **Cluster Manager** that is responsible for provisioning nodes in our cluster.\n",
    "  * Additional Cluster Managers are available for \n",
    "    <a href=\"https://spark.apache.org/docs/latest/running-on-mesos.html\" target=\"_blank\">Mesos</a>,\n",
    "    <a href=\"https://spark.apache.org/docs/latest/running-on-yarn.html\" target=\"_blank\">Yarn</a> and by other third parties.\n",
    "  * In addition to this, Spark has a <a href=\"https://spark.apache.org/docs/latest/spark-standalone.html\" target=\"_blank\">Standalone</a> mode in which you manually configure each node.\n",
    "* In each of these scenarios, the **Driver** is [presumably] running on one node, with each **Executors** running on N different nodes.\n",
    "\n",
    "One option, commonly used for local development, is to run Spark in **Local Mode**.\n",
    "* In **Local Mode**, both the **Driver** and one **Executor** share the same JVM.\n",
    "* This is an ideal scenario for experimentation, prototyping, and learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Spark Logo Tiny](https://www.quantiaconsulting.com/logos/logo_spark_tiny.png) Latest Release - Spark 3\n",
    "\n",
    "In June 2020 a new Spark major release (3.0.0) was made public.\n",
    "\n",
    "The major improvement of the new release are\n",
    "* Improved SQL Engine: 2x performance improvement on TPC-DS over Spark 2.4 \n",
    "    * Adaptive query execution (AQE)\n",
    "    * Dynamic partition pruning\n",
    "    * Improved ANSI SQL compliance\n",
    "    * Join Hints\n",
    "* Pandas APIs improvements:\n",
    "    * Python type hints\n",
    "    * Additional pandas UDFs type\n",
    "    * Improved Error Handling\n",
    "* Up to 40x speedups for calling R user-defined functions\n",
    "* ....\n",
    "\n",
    "[Official release page](https://spark.apache.org/releases/spark-release-3-0-0.html) with the complete list of improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ![Quantia Tiny Logo](https://www.quantiaconsulting.com/logos/quantia_logo_tiny.png) 2020 Quantia Consulting, srl. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "Apache Spark Overview",
  "notebookId": 422093619120039
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
