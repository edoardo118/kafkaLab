{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 2px;\">\n",
    "  <img src=\"https://www.quantiaconsulting.com/logos/quantia_logo_orizz.png\" alt=\"Quantia Consulting\" style=\"width: 600px; height: 250px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Avro Consumer \n",
    "\n",
    "**Technical Accomplishments:**\n",
    "- Consume data from Kafka Avro topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Let's start importing libraries and creating useful variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import os\n",
    "import qcutils\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import io\n",
    "from avro.io import DatumReader, BinaryDecoder\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.avro.functions import *\n",
    "import time\n",
    "import json\n",
    "import avro.schema\n",
    "import struct\n",
    "import requests \n",
    "\n",
    "spark_jars = qcutils.read_config_value(\"spark.additional-jars\").replace(\" \", \"\")\n",
    "\n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"quantia-edu\")\n",
    "    .config(\"spark.jars.packages\", spark_jars)\n",
    "    .getOrCreate()\n",
    "        )\n",
    "qcutils.init_spark_session(spark)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated Solution\n",
    "\n",
    "In this solution we use the `from_avro` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "\n",
    "topic = 'test-topic'\n",
    "\n",
    "assert len(topic) > 0, \"In order to avoid conflicts during write operation, please name the topic as <surname>-topic\"\n",
    "\n",
    "servers=qcutils.read_config_value(\"kafka.server\") + \":\" + str(qcutils.read_config_value(\"kafka.port\"))\n",
    "sr_url=qcutils.read_config_value(\"kafka.schema_registry.url\")\n",
    "\n",
    "sr_conf = {'url': sr_url}\n",
    "schema_registry = SchemaRegistryClient(sr_conf)\n",
    "\n",
    "value_schema = schema_registry.get_latest_version(topic+\"-value\").schema.schema_str\n",
    "\n",
    "print(value_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_df = (spark\n",
    "  .read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"subscribe\", topic)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"endingOffsets\", \"latest\")\n",
    "  .load())\n",
    "\n",
    "sc=\"\"\"\n",
    "{\n",
    "  \"namespace\": \"example.avro\",\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"PersonValue\",\n",
    "  \"fields\": \n",
    "    [{\n",
    "      \"name\": \"age\",\n",
    "      \"type\": \"int\"\n",
    "    }]\n",
    "}\n",
    "\"\"\"\n",
    "         \n",
    "ns_df.select(from_avro(ns_df.value, sc).alias(\"value\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", topic)\n",
    "  .load())\n",
    "\n",
    "output_df = (avro_df.select(from_avro(\"value\", value_schema).alias(\"v\")))\n",
    "\n",
    "query = (output_df\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\") \n",
    "    .option(\"path\", \"/home/jovyan/data/pyspark/integrated_solution.parquet\")\n",
    "    .option(\"checkpointLocation\",\"/home/jovyan/data/pyspark/checkpoint/integrated_solution\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"/home/jovyan/data/pyspark/integrated_solution.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this solution we use the `MessageSerializer`, an internal component of the `confluent_kafka` library, that hides most of the deserialization complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from confluent_kafka.avro.serializer.message_serializer import MessageSerializer\n",
    "from confluent_kafka.avro.cached_schema_registry_client import CachedSchemaRegistryClient\n",
    "\n",
    "@udf(\"string\")\n",
    "def from_avro_manual(value,sr_url): \n",
    "    sr_conf = {'url': sr_url}\n",
    "    schema_registry = CachedSchemaRegistryClient(sr_conf)\n",
    "    deSerializer = MessageSerializer(schema_registry)\n",
    "    return deSerializer.decode_message(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'test-topic'\n",
    "\n",
    "assert len(topic) > 0, \"In order to avoid conflicts during write operation, please name the topic as <surname>-topic\"\n",
    "\n",
    "servers=qcutils.read_config_value(\"kafka.server\") + \":\" + str(qcutils.read_config_value(\"kafka.port\"))\n",
    "sr_url=qcutils.read_config_value(\"kafka.schema_registry.url\")\n",
    "\n",
    "avro_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", topic)\n",
    "  .load())\n",
    "\n",
    "output_df = (avro_df.select(from_avro_manual(\"value\", lit(sr_url)).alias(\"v\")))\n",
    "\n",
    "query = (output_df\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\") \n",
    "    .option(\"path\", \"/home/jovyan/data/pyspark/manual_solution.parquet\")\n",
    "    .option(\"checkpointLocation\",\"/home/jovyan/data/pyspark/checkpoint/manual_solution\") \\\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"/home/jovyan/data/pyspark/manual_solution.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "\n",
    "data = [(1, Row(age=2, name='Alice'))]\n",
    "df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "avroDf = df.select(to_avro(df.value).alias(\"avro\"))\n",
    "avroDf.collect()\n",
    "\n",
    "jsonFormatSchema = '''{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":\n",
    "    [{\"name\":\"avro\",\"type\":[{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\n",
    "    \"fields\":[{\"name\":\"age\",\"type\":[\"long\",\"null\"]},\n",
    "    {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]},\"null\"]}]}'''\n",
    "\n",
    "avroDf.select(from_avro(avroDf.avro, jsonFormatSchema).alias(\"value\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ugly Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this solution we build the deserializer from scratch.\n",
    "\n",
    "In particular, in the `from_avro` udf, we have to take care of the:\n",
    "* `value` byte array parsing;\n",
    "* the extraction of the `magic byte` and of the `schema id` (the first 5 bytes of the encoded message);\n",
    "* the explicit request of the schema to the schema registry;\n",
    "* the message deserialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(\"string\")\n",
    "def from_avro(value,sr_base_url): \n",
    "    message_bytes = io.BytesIO(value)\n",
    "    \n",
    "    # extraction of the schema id and magic byte (5 bytes)\n",
    "    magic, schema_id = struct.unpack('>bI', message_bytes.read(5))\n",
    "    \n",
    "    # schema request\n",
    "    schemaRegistry = \"{}/schemas/ids/\".format(sr_base_url)\n",
    "    URL = schemaRegistry + str(schema_id)\n",
    "    r = requests.get(url = URL) \n",
    "    data = r.json() \n",
    "    avroSchema = str(data[\"schema\"])\n",
    "    schema = avro.schema.Parse(avroSchema)\n",
    "    \n",
    "    # decoding\n",
    "    reader = DatumReader(schema)\n",
    "    decoder = BinaryDecoder(message_bytes)\n",
    "    return reader.read(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = ''\n",
    "\n",
    "assert len(topic) > 0, \"In order to avoid conflicts during write operation, please name the topic as <surname>-topic\"\n",
    "\n",
    "servers=qcutils.read_config_value(\"kafka.server\") + \":\" + str(qcutils.read_config_value(\"kafka.port\"))\n",
    "sr_url=qcutils.read_config_value(\"kafka.schema_registry.url\")\n",
    "\n",
    "avro_df2 = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", topic)\n",
    "  .load())\n",
    "\n",
    "output_df2 = (avro_df2.select(from_avro(\"value\", lit(sr_url)).alias(\"v\")))\n",
    "\n",
    "query2 = (output_df2\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\") \n",
    "    .option(\"path\", \"/home/jovyan/data/pyspark/ugly_solution.parquet\")\n",
    "    .option(\"checkpointLocation\",\"/home/jovyan/data/pyspark/checkpoint/ugly_solution\") \\\n",
    "    .start())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"/home/jovyan/data/pyspark/ugly_solution.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ![Quantia Tiny Logo](https://www.quantiaconsulting.com/logos/quantia_logo_tiny.png) 2020 Quantia Consulting, srl. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "Ingesting from Kafka",
  "notebookId": 1507370365633587
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
