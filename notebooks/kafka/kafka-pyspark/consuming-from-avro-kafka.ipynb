{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 2px;\">\n",
    "  <img src=\"https://www.quantiaconsulting.com/logos/quantia_logo_orizz.png\" alt=\"Quantia Consulting\" style=\"width: 600px; height: 250px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Avro Consumer \n",
    "\n",
    "**Technical Accomplishments:**\n",
    "- Consume data from Kafka Avro topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Let's start importing libraries and creating useful variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import os\n",
    "import qcutils\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import io\n",
    "from avro.io import DatumReader, BinaryDecoder\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "import json\n",
    "import avro.schema\n",
    "import struct\n",
    "import requests \n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.5,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.5,org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,org.apache.kafka:kafka-clients:2.4.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.5 pyspark-shell'\n",
    "\n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "qcutils.init_spark_session(spark)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this solution we use the `MessageSerializer`, an internal component of the `confluent_kafka` library, that hides most of the deserialization complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from confluent_kafka.avro.serializer.message_serializer import MessageSerializer\n",
    "from confluent_kafka.avro.cached_schema_registry_client import CachedSchemaRegistryClient\n",
    "\n",
    "@udf(\"string\")\n",
    "def from_avro(value,sr_url): \n",
    "    sr_conf = {'url': sr_url}\n",
    "    schema_registry = CachedSchemaRegistryClient(sr_conf)\n",
    "    deSerializer = MessageSerializer(schema_registry)\n",
    "    return deSerializer.decode_message(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = ''\n",
    "\n",
    "assert len(topic) > 0, \"In order to avoid conflicts during write operation, please name the topic as <surname>-topic\"\n",
    "\n",
    "servers=qcutils.read_config_value(\"kafka.server\") + \":\" + str(qcutils.read_config_value(\"kafka.port\"))\n",
    "sr_url=qcutils.read_config_value(\"kafka.schema_registry.url\")\n",
    "\n",
    "avro_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", topic)\n",
    "  .load())\n",
    "\n",
    "output_df = (avro_df.select(from_avro(\"value\", lit(sr_url)).alias(\"v\")))\n",
    "\n",
    "query = (output_df\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\") \n",
    "    .option(\"path\", \"/home/jovyan/data/pyspark/best_solution.parquet\")\n",
    "    .option(\"checkpointLocation\",\"/home/jovyan/data/pyspark/checkpoint/best_solution\") \\\n",
    "    .start())\n",
    "\n",
    "#time.sleep(30)\n",
    "#dfw.stop()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ugly Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this solution we build the deserializer from scratch.\n",
    "\n",
    "In particular, in the `from_avro` udf, we have to take care of the:\n",
    "* `value` byte array parsing;\n",
    "* the extraction of the `magic byte` and of the `schema id` (the first 5 bytes of the encoded message);\n",
    "* the explicit request of the schema to the schema registry;\n",
    "* the message deserialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(\"string\")\n",
    "def from_avro(value,sr_base_url): \n",
    "    message_bytes = io.BytesIO(value)\n",
    "    \n",
    "    # extraction of the schema id and magic byte (5 bytes)\n",
    "    magic, schema_id = struct.unpack('>bI', message_bytes.read(5))\n",
    "    \n",
    "    # schema request\n",
    "    schemaRegistry = \"{}/schemas/ids/\".format(sr_base_url)\n",
    "    URL = schemaRegistry + str(schema_id)\n",
    "    r = requests.get(url = URL) \n",
    "    data = r.json() \n",
    "    avroSchema = str(data[\"schema\"])\n",
    "    schema = avro.schema.Parse(avroSchema)\n",
    "    \n",
    "    # decoding\n",
    "    reader = DatumReader(schema)\n",
    "    decoder = BinaryDecoder(message_bytes)\n",
    "    return reader.read(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = ''\n",
    "\n",
    "assert len(topic) > 0, \"In order to avoid conflicts during write operation, please name the topic as <surname>-topic\"\n",
    "\n",
    "servers=qcutils.read_config_value(\"kafka.server\") + \":\" + str(qcutils.read_config_value(\"kafka.port\"))\n",
    "sr_url=qcutils.read_config_value(\"kafka.schema_registry.url\")\n",
    "\n",
    "avro_df2 = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", topic)\n",
    "  .load())\n",
    "\n",
    "output_df2 = (avro_df2.select(from_avro(\"value\", lit(sr_url)).alias(\"v\")))\n",
    "\n",
    "query2 = (output_df2\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\") \n",
    "    .option(\"path\", \"/home/jovyan/data/pyspark/ugly_solution.parquet\")\n",
    "    .option(\"checkpointLocation\",\"/home/jovyan/data/pyspark/checkpoint/ugly_solution\") \\\n",
    "    .start())\n",
    "\n",
    "query2.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ![Quantia Tiny Logo](https://www.quantiaconsulting.com/logos/quantia_logo_tiny.png) 2020 Quantia Consulting, srl. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "Ingesting from Kafka",
  "notebookId": 1507370365633587
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
