{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 2px;\">\n",
    "  <img src=\"https://www.quantiaconsulting.com/logos/quantia_logo_orizz.png\" alt=\"Quantia Consulting\" style=\"width: 600px; height: 250px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data - Parquet Files\n",
    "\n",
    "**Preliminaries**\n",
    "The `read_parquet` is available from pandas 0.21. \n",
    "\n",
    "You need to install it manually, using the **Import Library** function. Search and install `pandas==0.23.4`\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "- Introduce the Parquet file format.\n",
    "- Read data from:\n",
    "  - Parquet files without a Schema.\n",
    "  - Parquet files with a Schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Python Tiny Logo](https://dl.dropboxusercontent.com/s/wl9nvyva3qjsaz2/logo_python_tiny.png) Getting Started\n",
    "\n",
    "Let's start importing libraries and creating useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import pandas\n",
    "import s3fs\n",
    "import boto3\n",
    "import qcutils\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "baseUri = \"s3://quantia-master/training/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![Python Tiny Logo](https://dl.dropboxusercontent.com/s/wl9nvyva3qjsaz2/logo_python_tiny.png) Reading from Parquet Files\n",
    "\n",
    "[Apache Parquet](https://parquet.apache.org/assets/img/parquet_logo.png) is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Parquet Files\n",
    "* Free & Open Source.\n",
    "* Increased query performance over row-based data stores.\n",
    "* Provides efficient data compression.\n",
    "* Designed for performance on large data sets.\n",
    "* Supports limited schema evolution.\n",
    "* Is a splittable \"file format\".\n",
    "* A <a href=\"https://en.wikipedia.org/wiki/Column-oriented_DBMS\" target=\"_blank\">Column-Oriented</a> data store\n",
    "\n",
    "**Row Format** \n",
    "\n",
    "| ID |  Name | Score |\n",
    "|:--:|:-----:|:-----:|\n",
    "| 1  | john  | 4.1   |\n",
    "| 2  | mike  | 3.5   |\n",
    "| 3  | sally | 6.4   |\n",
    "\n",
    "**Columnar View**\n",
    "\n",
    "```\n",
    "ID: 1, 2, 3\n",
    "Name: john, mike, sally\n",
    "Score: 4.1, 3.5, 6.4\n",
    "```\n",
    "\n",
    "**See also**:\n",
    "* [Apache Parquet](https://parquet.apache.org)\n",
    "* [Apache Parquet on Wijipedia](https://en.wikipedia.org/wiki/Apache_Parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "\n",
    "The data for this example shows the traffic to various articles on Wikipedia (<a href=\"https://dumps.wikimedia.org/other/pagecounts-raw\" target=\"_blank\">23 MB</a> from Wikipedia). \n",
    "\n",
    "The original file, captured August 5th of 2016 was downloaded, converted to a Parquet file\n",
    "\n",
    "**Note**: If the parquet files is partitioned (e.g. it was saved using spark), Pandas is unable to read it, but can only read the single part separately. A workaround to this problem, is to to read the separate fragments separately and then concatenate them.\n",
    "\n",
    "For this training we use a single fragment of the original parquet file: \n",
    "\n",
    "```\n",
    "s3://quantia-master/training/master-bip/training/wikipedia_pageviews_by_second.parquet/part-00000-tid-863803156164904753-537caea0-8c3b-4349-b236-0762d3215bce-184-c000.snappy.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike our CSV and JSON example, the parquet \"file\" is actually 11 files, 8 of which consist of the bulk of the data and the other three consist of meta-data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Parquet Files\n",
    "\n",
    "To read in this files, we will specify the location of the parquet directory.\n",
    "\n",
    "Let's try to read parquet file by passing the base location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = baseUri + \"wikipedia_pageviews_by_second.parquet\"\n",
    "tempDF = pandas.read_parquet(parquetFile)\n",
    "tempDF.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why did not it work?\n",
    "\n",
    "Pandas is not a distributed framework and it is not able to automatically concatenate the different parquet parts.\n",
    "\n",
    "Look in the folder on AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcutils.list_s3_bucket_objects(limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will read a single part of the parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = baseUri + \"wikipedia_pageviews_by_second.parquet/part-00000-tid-863803156164904753-537caea0-8c3b-4349-b236-0762d3215bce-184-c000.snappy.parquet\"\n",
    "\n",
    "tempDF = pandas.read_parquet(parquetFile)\n",
    "tempDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read only a subset of the columns\n",
    "We can read only specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDF = pandas.read_parquet(parquetFile, columns=['timestamp', 'requests'])\n",
    "tempDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: Reading from Parquet Files\n",
    "* We do not need to specify the schema - the column names and data types are stored in the parquet files.\n",
    "* Unlike the CSV or JSON readers that have to load the entire file and then infer the schema, the parquet reader can \"read\" the schema very quickly because it's reading that schema from the metadata.\n",
    "* It is possible to read only a subset of the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ![Quantia Tiny Logo](https://www.quantiaconsulting.com/logos/quantia_logo_tiny.png) 2020 Quantia Consulting, srl. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "Reading Data - Parquet",
  "notebookId": 1507370365633479
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
